{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1852881829.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install tensorflow\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n",
    "\n",
    "# a popular deep learning library\n",
    "# used for creating and training neutral networks, among other tasks related to machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "#  MNIST is a dataset of handwritten digits commonly used in image recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "#  This line loads the MNIST dataset. It splits the dataset into training data (train_images, train_labels) and test data (test_images, test_labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28\n",
    "# This likely relates to the size of the images in the MNIST dataset, which are 28x28 pixels. \n",
    "# Each image is represented as a 28x28 grid of grayscale pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(-1, 28*28)/255\n",
    "# This line reshapes the training images into a 2D array where each row represents an image. \n",
    "# The division by 255 is a normalization step, converting pixel values from the range [0, 255] to [0, 1].\n",
    "test_images = test_images.reshape(-1, 28*28)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape\n",
    "# This line of code is likely used to display the shape (dimensions) of the train_images array. \n",
    "# This helps understand how the data is structured before it's fed into a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building deep learning model\n",
    "\n",
    "dl_model = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Flatten(input_shape=(28*28,)),\n",
    "     tf.keras.layers.Dense(128, activation='relu'),\n",
    "     tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell constructs a sequential model using TensorFlow's Keras API.\n",
    "\n",
    "tf.keras.Sequential: This function is used to create a sequential model, where layers are added in sequence.\n",
    "\n",
    "Inside the sequential model, three layers are added:\n",
    "tf.keras.layers.Flatten(input_shape=(28*28,)): A flatten layer to convert the 2D 28x28 pixel images into a 1D array.\n",
    "\n",
    "tf.keras.layers.Dense(128, activation='relu'): A dense (fully connected) layer with 128 neurons and the ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "tf.keras.layers.Dense(10, activation='softmax'): Another dense layer with 10 neurons (one for each digit from 0 to 9) using the softmax activation function, which is typical for multi-class classification tasks like digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "dl_model.compile(optimizer = 'adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dl_model.compile(...): This command compiles the model with the following parameters:\n",
    "\n",
    "optimizer = 'adam': The Adam optimization algorithm is used, which is a popular choice for training neural networks.\n",
    "\n",
    "loss='sparse_categorical_crossentropy': This loss function is suitable for multi-class classification problems like MNIST.\n",
    "\n",
    "metrics = ['accuracy']: The model's performance will be evaluated based on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0132 - accuracy: 0.9961\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0116 - accuracy: 0.9964\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0082 - accuracy: 0.9977\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0100 - accuracy: 0.9969\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0075 - accuracy: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13d4c0350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line trains the model on the MNIST training data (train_images, train_labels) for 5 epochs. An epoch is one complete pass through the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "dl_test_loss, dl_test_acc = dl_model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells demonstrate the fundamental steps in building and training a deep learning model using TensorFlow: constructing the model architecture, compiling it with the desired optimizer and loss function, training on data, and evaluating its performance on a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = dl_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line uses the trained deep learning model to make predictions on the test dataset. The predict method outputs the model's predictions for each test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argmax function is used to find the indices of the maximum values along axis 1 (which represents each prediction's class probabilities). This effectively converts the softmax outputs to class labels, determining which digit the model predicts for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes\n",
    "# This line likely displays the array of predicted classes for the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9778"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted_classes,test_labels)\n",
    "# This computes the accuracy of the deep learning model by comparing the predicted classes with the actual labels from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(train_images, train_labels)\n",
    "rf_pred = rf_model.predict(test_images)\n",
    "accuracy_score(rf_pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100): Creates a RandomForestClassifier with 100 trees.\n",
    "\n",
    "rf_model.fit(train_images, train_labels): Trains the random forest model on the training data.\n",
    "\n",
    "rf_pred = rf_model.predict(test_images): Makes predictions on the test dataset.\n",
    "\n",
    "accuracy_score(rf_pred, test_labels): Computes the accuracy of the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(train_images, train_labels)\n",
    "lr_pred = lr_model.predict(test_images)\n",
    "accuracy_score(lr_pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_model = LogisticRegression(max_iter=1000): Creates a LogisticRegression model with a maximum of 1000 iterations for the solver.\n",
    "\n",
    "lr_model.fit(train_images, train_labels): Trains the logistic regression model on the training data.\n",
    "\n",
    "lr_pred = lr_model.predict(test_images): Uses the model to make predictions on the test dataset.\n",
    "\n",
    "accuracy_score(lr_pred, test_labels): Calculates the accuracy of the logistic regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
