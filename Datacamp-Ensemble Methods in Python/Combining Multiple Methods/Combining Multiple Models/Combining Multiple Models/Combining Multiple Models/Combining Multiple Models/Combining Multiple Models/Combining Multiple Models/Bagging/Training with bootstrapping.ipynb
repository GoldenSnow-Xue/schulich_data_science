{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap aggregating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heterogeneous vs Homogeneous Ensembles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogeneous:\n",
    "Different algorithms (fine_tuned)\n",
    "Small amount of estimators\n",
    "Voting, Averaging, and Stacking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogeneous:\n",
    "The same algotithm (\"weak\" model)\n",
    "Large amout of estimators\n",
    "Bagging and Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condorcet's Jury Theorem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements:\n",
    "1. Models are independent.\n",
    "2. Each model performs better than random guessing.\n",
    "3. All individual models have similar performance.\n",
    "\n",
    "Conclusion: Adding more models imporves the performance of the ensemble(Voting or Averging), and this approaches 1 (100%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping requires:\n",
    "1. Random subsamples.\n",
    "2. Using replacement.\n",
    "\n",
    "# Bootstrapping guarantees:\n",
    "1. Diverse crowd: different datasets\n",
    "2. Independent: separately sampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros and cons of bagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros\n",
    "1. Bagging usually reduces variance\n",
    "2. Overfitting can be avoided by the ensemble itself\n",
    "3. More stability and robustness\n",
    "\n",
    "# Cons\n",
    "1. It is computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample with replacement\n",
    "X_train_sample = X_train.sample(frac=1.0, replace=True, random_state=42)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]\n",
    "\n",
    "# Build a \"weak\" Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=500)\n",
    "\n",
    "# Fit the model to the training sample\n",
    "clf.fit(X_train_sample, y_train_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
